{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:  # Check platform (Colab or Jupyter)\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PATH = \"/content/drive/My Drive/joklar/\"\n",
    "except:\n",
    "    PATH = os.path.expanduser(\"~\") + \"/drive/joklar/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # Check platform (Colab)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PATH = \"/content/drive/My Drive/joklar/\"\n",
    "except ImportError:  # Not in Colab\n",
    "    host = os.environ.get('HOST')\n",
    "    if host == 'makki':\n",
    "        PATH = os.path.expanduser(\"~/drive/joklar/\")\n",
    "    elif host == 'elja':\n",
    "        PATH = os.path.expanduser(\"~/joklar/\")\n",
    "    else:\n",
    "        s = f\"$HOST is {host}, it should be 'makki' or 'elja'\"\n",
    "        raise EnvironmentError(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Working directory path set to:\", PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os, sys, time, pandas as pd, tensorflow as tf, random\n",
    "start_time = time.time()\n",
    "import keras\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(PATH + \"src\")\n",
    "from util.util import install_import\n",
    "from util.image_util import load_data, compactify\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[3]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model type and data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELTYPE = \"unet\"\n",
    "AUGMENTATION = False\n",
    "MODEL_PATH = PATH + \"results/\" + MODELTYPE + \"/\"\n",
    "DATA_PATH = PATH + \"data/lang/\"\n",
    "COMPACT = True\n",
    "COMBINE_TEST_VAL = True\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "os.chdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[4]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import project-specific packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES<br>\n",
    "deeplab-v3+ is copied more or less directly from the GitHub repository<br>\n",
    "   github.com/david8862/tf-keras-deeplabv3p-model-set<br>\n",
    "(the original files are in the subdirectory from_github, cf differences.txt)<br>\n",
    "<br>\n",
    "unet is copied from..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELTYPE == \"unet\":\n",
    "    from unet.unet import get_unet\n",
    "else:  # deeplab\n",
    "    install_import(\"keras_applications\")\n",
    "    from deeplabv3p.model import get_deeplabv3p_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[5]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(indices):\n",
    "    # Define data split (training, validation, and test sets)\n",
    "    seed = 41\n",
    "    tf_seed = 41\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(tf_seed)\n",
    "    test_size = 0.15\n",
    "    train_size = 0.18  # or 0.15/(1 - test_size)\n",
    "    temp, test = train_test_split(indices, test_size=test_size, random_state=seed)\n",
    "    train, val = train_test_split(temp, test_size=train_size, random_state=seed)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(image, mask, *_) = load_data(DATA_PATH + \"data.npz\", \"border\")\n",
    "npixel = 256\n",
    "if COMPACT:\n",
    "    # Compact data by a factor of fold**3\n",
    "    fold = 4\n",
    "    npixel //= fold\n",
    "    image = compactify(image, fold=fold)\n",
    "    mask = compactify(mask, fold=fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = image.dtype\n",
    "ntile = len(image)\n",
    "nchan = image.shape[-1]\n",
    "print(f\"{nchan} channels, {ntile} tiles, datatype: {dtype}\")\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Mask shape:\", mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[7]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "class PrintCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 3 != 0: return\n",
    "        logs = logs or {}\n",
    "        output = f\"Epoch {epoch + 1}/{self.params['epochs']}: \"\n",
    "        output += \", \".join([f\"{k}={v:.4f}\" for k, v in logs.items()])\n",
    "        print(output)\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        return\n",
    "        logs = logs or {}\n",
    "        output = f\"Batch {batch + 1}/{self.params['steps']}: \"\n",
    "        output += \", \".join([f\"{k}={v:.4f}\" for k, v in logs.items()])\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[8]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def albumentations_generator(img, mask, train):\n",
    "    # Implement data augmentation with the albumentations package\n",
    "    npixel = img.shape[1]\n",
    "    augmentation = {\n",
    "        \"random_gamma_probability\": 0.5,\n",
    "        \"random_gamma_gamma_limit\": [80, 120],\n",
    "        \"flipud_probability\":       0.5,\n",
    "        \"fliplr_probability\":       0.5,\n",
    "        \"rotate90_probability\":     0.5,\n",
    "        \"random_crop_probability\":  0.5,\n",
    "        \"random_crop_height\":       npixel,\n",
    "        \"random_crop_width\":        npixel,\n",
    "        \"random_crop_scale_x\":      0.5,\n",
    "        \"random_crop_scale_y\":      0.5\n",
    "    }\n",
    "    from util.generator import AugmentDataGenerator\n",
    "    train_gen = AugmentDataGenerator(img[train], mask[train], augmentation)\n",
    "    return train_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[9]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(npixel, nchan, init_lr=1e-4):\n",
    "    if MODELTYPE == \"unet\":\n",
    "        input_img = keras.layers.Input((npixel, npixel, nchan), name='img')\n",
    "        model = get_unet(input_img, n_filters=16, dropout=0.3, batchnorm=True)\n",
    "    else:  # deeplab\n",
    "        get_deeplab = get_deeplabv3p_model\n",
    "        model = get_deeplab(model_type='resnet50', num_classes=1,\n",
    "                            model_input_shape=(npixel, npixel),\n",
    "                            output_stride=16,\n",
    "                            freeze_level=0,\n",
    "                            weights_path=None,\n",
    "                            training=True,\n",
    "                            use_subpixel=False)\n",
    "    Adam_params = {\"learning_rate\": init_lr, \"clipnorm\": 1.0}\n",
    "    model.compile(optimizer=Adam(**Adam_params),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    #model.save('model_first.keras')\n",
    "    return model\n",
    "    # NOTE: Saving weights only gives a file just as big as saving the whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[10]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_callbacks(factor=0.3, patience=10, min_lr=1e-5):\n",
    "    checkpoint_best = ModelCheckpoint('model_best.keras',\n",
    "                                      monitor=\"val_loss\",\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "    checkpoint_last = ModelCheckpoint('model_last.keras',\n",
    "                                      save_weights_only=True)\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=60),\n",
    "        ReduceLROnPlateau(factor=factor, patience=patience, min_lr=min_lr),\n",
    "        checkpoint_best,\n",
    "        checkpoint_last,\n",
    "        PrintCallback(),\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[11]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, val, test) = train_val_test_split(range(ntile))\n",
    "if COMBINE_TEST_VAL:\n",
    "    val += test\n",
    "if AUGMENTATION:\n",
    "    data_input = (albumentations_generator(image, mask, train),)\n",
    "else:\n",
    "    data_input = (image[train], mask[train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [0.1, 0.3]\n",
    "patiences = [5, 10, 20]\n",
    "min_lrs = [1e-5, 1e-6]\n",
    "init_lrs = [1e-3, 1e-4]\n",
    "batch_sizes = [8, 32, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for factor in factors:\n",
    "    for patience in patiences:\n",
    "        for min_lr in min_lrs:\n",
    "            for init_lr in init_lrs:\n",
    "                for batch_size in batch_sizes:\n",
    "                    callbacks = define_callbacks(factor, patience, min_lr)\n",
    "                    model = create_model(npixel, nchan, init_lr=init_lr)\n",
    "                    # Train (save best results, and possibly all)\n",
    "                    results = model.fit(*data_input,\n",
    "                        verbose=0,\n",
    "                        batch_size=8,\n",
    "                        epochs=10,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=(image[val], mask[val]))\n",
    "                    print(results)\n",
    "                    pass\n",
    "history = results.history\n",
    "len(history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.argmin(history[\"val_loss\"]),\n",
    "         np.min(history[\"val_loss\"]),\n",
    "         marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute predicted probabilites everywhere; evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict(image, verbose=1)\n",
    "(test_loss, test_accuracy) = model.evaluate(image[test], mask[test])\n",
    "print(f'Accuracy on test: {test_accuracy}')\n",
    "plt.hist(probs.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the training history and model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdhistory = pd.DataFrame(history)\n",
    "pdhistory.to_csv(\"result_history.csv\")\n",
    "np.savez(\"probs.npz\", probs, test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display running time and disconnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "min, sec = divmod(int(end_time - start_time), 60)\n",
    "print(f\"Total execution time: {min}:{sec:02}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[ ]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
